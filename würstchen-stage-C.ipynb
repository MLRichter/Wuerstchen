{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6200fa6-a1b7-42f1-a5dc-8eee985e0123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from vqgan import VQModel\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, T5EncoderModel, CLIPTextModel\n",
    "from torch.utils.data import DataLoader\n",
    "from modules import Paella, EfficientNetEncoder, Prior\n",
    "from diffuzz import Diffuzz\n",
    "import transformers\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def showimages(imgs, rows=False, **kwargs):\n",
    "    plt.figure(figsize=(kwargs.get(\"width\", 32), kwargs.get(\"height\", 32)))\n",
    "    plt.axis(\"off\")\n",
    "    if rows:\n",
    "        plt.imshow(torch.cat([torch.cat([i for i in row], dim=-1) for row in imgs], dim=-2).permute(1, 2, 0).cpu())\n",
    "    else:\n",
    "        plt.imshow(torch.cat([torch.cat([i for i in imgs], dim=-1)], dim=-2).permute(1, 2, 0).cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad86d3d3-37bd-484c-8860-717cfef480e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, model_inputs, latent_shape, unconditional_inputs=None, init_x=None, steps=12, renoise_steps=None, temperature = (0.7, 0.3), cfg=(8.0, 8.0), mode = 'multinomial', t_start=1.0, t_end=0.0, sampling_conditional_steps=None, sampling_quant_steps=None, attn_weights=None): # 'quant', 'multinomial', 'argmax'\n",
    "    device = unconditional_inputs[\"byt5\"].device\n",
    "    if sampling_conditional_steps is None:\n",
    "        sampling_conditional_steps = steps\n",
    "    if sampling_quant_steps is None:\n",
    "        sampling_quant_steps = steps\n",
    "    if renoise_steps is None:\n",
    "        renoise_steps = steps-1\n",
    "    if unconditional_inputs is None:\n",
    "        unconditional_inputs = {k: torch.zeros_like(v) for k, v in model_inputs.items()}\n",
    "    intermediate_images = []\n",
    "    # with torch.inference_mode():\n",
    "    init_noise = torch.randint(0, model.num_labels, size=latent_shape, device=device)\n",
    "    if init_x != None:\n",
    "        sampled = init_x\n",
    "    else:\n",
    "        sampled = init_noise.clone()\n",
    "    t_list = torch.linspace(t_start, t_end, steps+1)\n",
    "    temperatures = torch.linspace(temperature[0], temperature[1], steps)\n",
    "    cfgs = torch.linspace(cfg[0], cfg[1], steps)\n",
    "    if cfg is not None:\n",
    "        model_inputs = {k:torch.cat([v, v_u]) for (k, v), (k_u, v_u) in zip(model_inputs.items(), unconditional_inputs.items())}\n",
    "    for i, tv in enumerate(t_list[:steps]):\n",
    "        if i >= sampling_quant_steps:\n",
    "            mode = \"quant\"\n",
    "        t = torch.ones(latent_shape[0], device=device) * tv\n",
    "\n",
    "        if cfg is not None and i < sampling_conditional_steps:\n",
    "            logits, uncond_logits = model(torch.cat([sampled]*2), torch.cat([t]*2), **model_inputs).chunk(2)\n",
    "            logits = logits * cfgs[i] + uncond_logits * (1-cfgs[i])\n",
    "        else:\n",
    "            logits = model(sampled, t, **model_inputs)\n",
    "\n",
    "        scores = logits.div(temperatures[i]).softmax(dim=1)\n",
    "\n",
    "        if mode == 'argmax':\n",
    "            sampled = logits.argmax(dim=1)\n",
    "        elif mode == 'multinomial':\n",
    "            sampled = scores.permute(0, 2, 3, 1).reshape(-1, logits.size(1))\n",
    "            sampled = torch.multinomial(sampled, 1)[:, 0].view(logits.size(0), *logits.shape[2:])\n",
    "        elif mode == 'quant':\n",
    "            sampled = scores.permute(0, 2, 3, 1) @ vqmodel.vquantizer.codebook.weight.data\n",
    "            sampled = vqmodel.vquantizer.forward(sampled, dim=-1)[-1]\n",
    "        else:\n",
    "            raise Exception(f\"Mode '{mode}' not supported, use: 'quant', 'multinomial' or 'argmax'\")\n",
    "\n",
    "        intermediate_images.append(sampled)\n",
    "\n",
    "        if i < renoise_steps:\n",
    "            t_next = torch.ones(latent_shape[0], device=device) * t_list[i+1]\n",
    "            sampled = model.add_noise(sampled, t_next, random_x=init_noise)[0]\n",
    "            intermediate_images.append(sampled)\n",
    "    return sampled, intermediate_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f2fc64-1c37-4a23-9893-625130c42665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using EffNet L.\n"
     ]
    }
   ],
   "source": [
    "effnet_preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(384, interpolation=torchvision.transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    torchvision.transforms.CenterCrop(384),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "def encode(x):\n",
    "    return vqmodel.encode(x, quantize=True)[2]\n",
    "    \n",
    "def decode(img_seq):\n",
    "    return vqmodel.decode_indices(img_seq)\n",
    "        \n",
    "def embed_clip(caption, negative_caption=\"\", batch_size=4, device=\"cuda\"):\n",
    "    clip_tokens = clip_tokenizer([caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
    "    clip_text_embeddings = clip_model(**clip_tokens).last_hidden_state\n",
    "\n",
    "    clip_tokens_uncond = clip_tokenizer([negative_caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
    "    clip_text_embeddings_uncond = clip_model(**clip_tokens_uncond).last_hidden_state\n",
    "    return clip_text_embeddings, clip_text_embeddings_uncond\n",
    "\n",
    "vqmodel = VQModel().to(device)\n",
    "vqmodel.load_state_dict(torch.load(\"models/vqgan_f4_v1_500k.pt\", map_location=device)[\"state_dict\"])\n",
    "vqmodel.eval().requires_grad_(False)\n",
    "\n",
    "clip_model = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\").to(device).eval().requires_grad_(False)\n",
    "clip_tokenizer = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "\n",
    "pretrained_checkpoint = torch.load(\"models/model_stage_b.pt\", map_location=device)\n",
    "\n",
    "effnet = EfficientNetEncoder(effnet=\"efficientnet_v2_l\").to(device)\n",
    "effnet.load_state_dict(pretrained_checkpoint['effnet_state_dict'])\n",
    "effnet.eval().requires_grad_(False)\n",
    "\n",
    "diffuzz = Diffuzz(device=device)\n",
    "\n",
    "# - Paella Model as generator - \n",
    "generator = Paella(byt5_embd=1024).to(device)\n",
    "generator.load_state_dict(pretrained_checkpoint['state_dict'])\n",
    "generator.eval().requires_grad_(False)\n",
    "del pretrained_checkpoint\n",
    "\n",
    "checkpoint = torch.load(\"models/model_stage_c.pt\", map_location=device)\n",
    "model = Prior(c_in=16, c=1536, c_cond=1024, c_r=64, depth=32, nhead=24).to(device)\n",
    "model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "model.eval().requires_grad_(False)\n",
    "del checkpoint\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cd3ff0-3354-4cf9-8813-9856ff1da66d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True)\n",
    "generator = torch.compile(generator, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74257762-549f-4981-8b1d-bd7118d38f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "caption = \"\"\n",
    "negative_caption = \"low resolution, low detail, bad quality, blurry\"\n",
    "# negative_caption = \"\"\n",
    "prior_timesteps = 60\n",
    "prior_cfg = 6\n",
    "prior_sampler = \"ddpm\"\n",
    "\n",
    "clip_text_embeddings, clip_text_embeddings_uncond = embed_clip(caption, negative_caption, batch_size, device)\n",
    "\n",
    "effnet_features_shape = (batch_size, 16, 12, 12)\n",
    "effnet_embeddings_uncond = torch.zeros(effnet_features_shape).to(device)\n",
    "generator_latent_shape = (batch_size, 128, 128)\n",
    "# torch.manual_seed(42)\n",
    "with torch.cuda.amp.autocast(dtype=torch.bfloat16), torch.no_grad():\n",
    "    s = time.time()\n",
    "    sampled = diffuzz.sample(model, {'c': clip_text_embeddings}, unconditional_inputs={\"c\": clip_text_embeddings_uncond}, shape=effnet_features_shape,\n",
    "                            timesteps=prior_timesteps, cfg=prior_cfg, sampler=prior_sampler,\n",
    "                            t_start=1.0)[-1]\n",
    "    print(f\"Prior Sampling: {time.time() - s}\")\n",
    "    temperature, cfg, steps =(1.0, 0.6), (2.0, 2.0), 8\n",
    "    s = time.time()\n",
    "    sampled_images_original, intermediate = sample(\n",
    "        generator, {'effnet': sampled,'byt5': clip_text_embeddings}, generator_latent_shape, unconditional_inputs = {'effnet': effnet_embeddings_uncond, 'byt5': clip_text_embeddings_uncond},\n",
    "        temperature=temperature, cfg=cfg, steps=steps\n",
    "    )\n",
    "    print(f\"Generator Sampling: {time.time() - s}\")\n",
    "\n",
    "s = time.time()\n",
    "sampled = decode(sampled_images_original)\n",
    "print(f\"Decoder Generation: {time.time() - s}\")\n",
    "# intermediate = [decode(i) for i in intermediate]\n",
    "print(f\"Temperature: {temperature}, CFG: {cfg}, Steps: {steps}\")\n",
    "print(caption)\n",
    "showimages(sampled)\n",
    "# showimages(intermediate, rows=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
